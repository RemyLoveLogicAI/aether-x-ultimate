# ==============================================================================
# vLLM Docker Configuration - Environment Variables
# ==============================================================================
# Copy this file to .env and customize for your deployment
# ==============================================================================

# ------------------------------------------------------------------------------
# vLLM Version Configuration
# ------------------------------------------------------------------------------
# Available versions: latest, v0.11.1, v0.11.0, v0.10.4, etc.
# See: https://hub.docker.com/r/vllm/vllm-openai/tags
# SECURITY: Using v0.11.1+ to address CVE-2025-66448 RCE vulnerability
VLLM_VERSION=v0.11.1

# ------------------------------------------------------------------------------
# GPU Configuration
# ------------------------------------------------------------------------------
# Number of GPUs to use
GPU_COUNT=1

# Specific GPU IDs (comma-separated, e.g., "0,1,2")
CUDA_VISIBLE_DEVICES=0

# GPU memory utilization (0.0 to 1.0)
GPU_MEMORY_UTILIZATION=0.9

# ------------------------------------------------------------------------------
# Model Configuration
# ------------------------------------------------------------------------------
# Hugging Face model name or local path
MODEL_NAME=meta-llama/Llama-3.1-8B-Instruct

# Custom model name for API
SERVED_MODEL_NAME=aether-x-ultimate

# Maximum sequence length
MAX_MODEL_LEN=4096

# SECURITY: Enable trust remote code ONLY for trusted models
# Setting to false by default prevents arbitrary code execution
# Only enable if you fully trust the model source
TRUST_REMOTE_CODE=false

# ------------------------------------------------------------------------------
# Attention Backend
# ------------------------------------------------------------------------------
# Options: FLASHINFER, FLASH_ATTN, XFORMERS, TORCH_SDPA
ATTENTION_BACKEND=FLASHINFER

# ------------------------------------------------------------------------------
# API Configuration
# ------------------------------------------------------------------------------
# API port for vLLM OpenAI-compatible server
VLLM_PORT=8000

# Metrics endpoint port
METRICS_PORT=8001

# Application server port
APP_PORT=8080

# API key for authentication (optional, leave empty for no auth)
VLLM_API_KEY=

# ------------------------------------------------------------------------------
# Directory Configuration
# ------------------------------------------------------------------------------
# Local directory for model cache
MODEL_CACHE_DIR=./models

# Local directory for logs
LOGS_DIR=./logs

# Local directory for configuration files
CONFIG_DIR=./config

# ------------------------------------------------------------------------------
# Hugging Face Configuration
# ------------------------------------------------------------------------------
# Your Hugging Face token (required for gated models)
HUGGINGFACE_TOKEN=

# Hugging Face cache directory
HF_HOME=/root/.cache/huggingface

# ------------------------------------------------------------------------------
# Performance Tuning
# ------------------------------------------------------------------------------
# Number of parallel build jobs
MAX_JOBS=4

# CUDA architectures to compile for
TORCH_CUDA_ARCH_LIST=8.0;8.6;8.9;9.0

# Enable tensor parallelism (for multi-GPU)
TENSOR_PARALLEL_SIZE=1

# Enable pipeline parallelism (for multi-GPU)
PIPELINE_PARALLEL_SIZE=1

# ------------------------------------------------------------------------------
# Application Environment
# ------------------------------------------------------------------------------
# Application environment (development, staging, production)
APP_ENV=production

# Log level (debug, info, warning, error, critical)
LOG_LEVEL=info

# ------------------------------------------------------------------------------
# Monitoring Configuration (Optional)
# ------------------------------------------------------------------------------
# Prometheus port
PROMETHEUS_PORT=9090

# Grafana port
GRAFANA_PORT=3000

# SECURITY: DO NOT use default passwords in production!
# Set a strong password here or in your secrets management system
# Leave empty to be prompted on first login
GRAFANA_PASSWORD=

# ------------------------------------------------------------------------------
# Advanced vLLM Configuration
# ------------------------------------------------------------------------------
# Swap space in GiB (for offloading)
SWAP_SPACE=4

# Enable prefix caching
ENABLE_PREFIX_CACHING=true

# Quantization method (none, awq, gptq, squeezellm, fp8)
QUANTIZATION=

# KV cache data type (auto, fp8, fp16, bf16)
KV_CACHE_DTYPE=auto

# Enable chunked prefill
ENABLE_CHUNKED_PREFILL=false

# Max number of batched tokens
MAX_NUM_BATCHED_TOKENS=

# Max number of sequences
MAX_NUM_SEQS=256

# ------------------------------------------------------------------------------
# CUDA Version (for reference, managed by base image)
# ------------------------------------------------------------------------------
# The official vLLM images come with:
# - vLLM v0.11.1+: CUDA 12.9.1, torch 2.9.1
# - vLLM latest: Latest compatible versions
# No need to manage CUDA/torch versions manually!
# ==============================================================================
